{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import csv, json\n",
    "from zipfile import ZipFile\n",
    "from os.path import expanduser, exists\n",
    "import pandas as pd\n",
    "import datetime, time, json\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization, Convolution1D, Merge, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_PAIRS_FILE = 'data/train.csv'\n",
    "GLOVE_FILE = 'data/glove.840B.300d.txt'\n",
    "Q1_TRAINING_DATA_FILE = 'data/q1_train.npy'\n",
    "Q2_TRAINING_DATA_FILE = 'data/q2_train.npy'\n",
    "LABEL_TRAINING_DATA_FILE = 'data/label_train.npy'\n",
    "WORD_EMBEDDING_MATRIX_FILE = 'data/word_embedding_matrix.npy'\n",
    "NB_WORDS_DATA_FILE = 'data/nb_words.json'\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "SAFE_DIV = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question pairs: 404290\n"
     ]
    }
   ],
   "source": [
    "question1 = []\n",
    "question2 = []\n",
    "is_duplicate = []\n",
    "with open(QUESTION_PAIRS_FILE, encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        question1.append(preprocess(row['question1']))\n",
    "        question2.append(preprocess(row['question2']))\n",
    "        is_duplicate.append(row['is_duplicate'])\n",
    "print('Question pairs: %d' % len(question1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in index: 91458\n"
     ]
    }
   ],
   "source": [
    "questions = question1 + question2\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(questions)\n",
    "question1_word_sequences = tokenizer.texts_to_sequences(question1)\n",
    "question2_word_sequences = tokenizer.texts_to_sequences(question2)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 24937\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of question1 data tensor: (404290, 25)\n",
      "Shape of question2 data tensor: (404290, 25)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(is_duplicate, dtype=int)\n",
    "print('Shape of question1 data tensor:', q1_data.shape)\n",
    "print('Shape of question2 data tensor:', q2_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open(Q1_TRAINING_DATA_FILE, 'wb'), q1_data)\n",
    "np.save(open(Q2_TRAINING_DATA_FILE, 'wb'), q2_data)\n",
    "np.save(open(LABEL_TRAINING_DATA_FILE, 'wb'), labels)\n",
    "np.save(open(WORD_EMBEDDING_MATRIX_FILE, 'wb'), word_embedding_matrix)\n",
    "with open(NB_WORDS_DATA_FILE, 'w') as f:\n",
    "    json.dump({'nb_words': nb_words}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Variables\n",
    "MODEL_WEIGHTS_FILE = 'question_pairs_weights_nn_base.h5'\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.5\n",
    "RNG_SEED = 13371447\n",
    "NB_EPOCHS = 1\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 32\n",
    "nb_filter = 32 # Number of filters to use in Convolution1D\n",
    "filter_length = 3 # Length of filter for Convolution1D\n",
    "SENT_EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precision_score = precision(y_true, y_pred)\n",
    "    recall_score = recall(y_true, y_pred)\n",
    "    return 2*((precision_score*recall_score)/(precision_score+recall_score+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1_train 202145\n",
      "Q2_train 202145\n",
      "Q1_test 202145\n",
      "Q2_test 202145\n",
      "Starting training at 2018-05-08 01:53:59.021398\n",
      "Train on 181930 samples, validate on 20215 samples\n",
      "Epoch 1/1\n",
      " - 35s - loss: 0.5471 - acc: 0.7229 - precision: 0.6644 - recall: 0.5201 - f1: 0.5657 - val_loss: 0.5287 - val_acc: 0.7440 - val_precision: 0.6971 - val_recall: 0.5297 - val_f1: 0.5891\n",
      "Training ended at 2018-05-08 01:54:36.358207\n",
      "Minutes elapsed: 0.622267\n",
      "Maximum accuracy at epoch 1 = 0.7440\n",
      "loss = 0.5292, accuracy = 0.7398,precision = 0.6956, recall = 0.5218,f1 = 0.5842\n"
     ]
    }
   ],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]\n",
    "print(\"Q1_train\" , len(Q1_train))\n",
    "print(\"Q2_train\" , len(Q2_train))\n",
    "print(\"Q1_test\" , len(Q1_test))\n",
    "print(\"Q2_test\" , len(Q2_test))\n",
    "\n",
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question1)\n",
    "\n",
    "q1 = Dense(EMBEDDING_DIM,  activation='relu')(q1)\n",
    "q1 = Flatten()(q1)\n",
    "#q3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q3)\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question2)\n",
    "q2  = Dense(EMBEDDING_DIM,  activation='relu')(q2)\n",
    "q2 = Flatten()(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',precision,recall,f1])\n",
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))\n",
    "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc']))\n",
    "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))\n",
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "loss, accuracy,precision,recall, f1 = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
    "print('loss = {0:.4f}, accuracy = {1:.4f},precision = {2:.4f}, recall = {3:.4f},f1 = {4:.4f}'.format(loss, accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1_train 242574\n",
      "Q2_train 242574\n",
      "Q1_test 161716\n",
      "Q2_test 161716\n",
      "Starting training at 2018-05-08 01:55:57.072297\n",
      "Train on 218316 samples, validate on 24258 samples\n",
      "Epoch 1/1\n",
      " - 41s - loss: 0.5436 - acc: 0.7272 - precision: 0.6685 - recall: 0.5277 - f1: 0.5730 - val_loss: 0.5239 - val_acc: 0.7425 - val_precision: 0.6826 - val_recall: 0.5652 - val_f1: 0.6066\n",
      "Training ended at 2018-05-08 01:56:40.666570\n",
      "Minutes elapsed: 0.726564\n",
      "Maximum accuracy at epoch 1 = 0.7425\n",
      "loss = 0.5241, accuracy = 0.7413,precision = 0.6789, recall = 0.5659,f1 = 0.6057\n"
     ]
    }
   ],
   "source": [
    "TEST_SPLIT = 0.4\n",
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]\n",
    "print(\"Q1_train\" , len(Q1_train))\n",
    "print(\"Q2_train\" , len(Q2_train))\n",
    "print(\"Q1_test\" , len(Q1_test))\n",
    "print(\"Q2_test\" , len(Q2_test))\n",
    "\n",
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question1)\n",
    "\n",
    "q1 = Dense(EMBEDDING_DIM,  activation='relu')(q1)\n",
    "q1 = Flatten()(q1)\n",
    "#q3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q3)\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question2)\n",
    "q2  = Dense(EMBEDDING_DIM,  activation='relu')(q2)\n",
    "q2 = Flatten()(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',precision,recall,f1])\n",
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))\n",
    "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc']))\n",
    "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))\n",
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "loss, accuracy,precision,recall, f1 = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
    "print('loss = {0:.4f}, accuracy = {1:.4f},precision = {2:.4f}, recall = {3:.4f},f1 = {4:.4f}'.format(loss, accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1_train 283003\n",
      "Q2_train 283003\n",
      "Q1_test 121287\n",
      "Q2_test 121287\n",
      "Starting training at 2018-05-08 01:57:22.212299\n",
      "Train on 254702 samples, validate on 28301 samples\n",
      "Epoch 1/1\n",
      " - 49s - loss: 0.5401 - acc: 0.7294 - precision: 0.6724 - recall: 0.5314 - f1: 0.5772 - val_loss: 0.5212 - val_acc: 0.7478 - val_precision: 0.6888 - val_recall: 0.5732 - val_f1: 0.6140\n",
      "Training ended at 2018-05-08 01:58:13.328690\n",
      "Minutes elapsed: 0.851935\n",
      "Maximum accuracy at epoch 1 = 0.7478\n",
      "loss = 0.5218, accuracy = 0.7456,precision = 0.6893, recall = 0.5650,f1 = 0.6095\n"
     ]
    }
   ],
   "source": [
    "TEST_SPLIT = 0.3\n",
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]\n",
    "print(\"Q1_train\" , len(Q1_train))\n",
    "print(\"Q2_train\" , len(Q2_train))\n",
    "print(\"Q1_test\" , len(Q1_test))\n",
    "print(\"Q2_test\" , len(Q2_test))\n",
    "\n",
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question1)\n",
    "\n",
    "q1 = Dense(EMBEDDING_DIM,  activation='relu')(q1)\n",
    "q1 = Flatten()(q1)\n",
    "#q3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q3)\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question2)\n",
    "q2  = Dense(EMBEDDING_DIM,  activation='relu')(q2)\n",
    "q2 = Flatten()(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',precision,recall,f1])\n",
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))\n",
    "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc']))\n",
    "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))\n",
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "loss, accuracy,precision,recall, f1 = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
    "print('loss = {0:.4f}, accuracy = {1:.4f},precision = {2:.4f}, recall = {3:.4f},f1 = {4:.4f}'.format(loss, accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1_train 323432\n",
      "Q2_train 323432\n",
      "Q1_test 80858\n",
      "Q2_test 80858\n",
      "Starting training at 2018-05-08 01:58:55.915810\n",
      "Train on 291088 samples, validate on 32344 samples\n",
      "Epoch 1/1\n",
      " - 55s - loss: 0.5390 - acc: 0.7302 - precision: 0.6726 - recall: 0.5335 - f1: 0.5782 - val_loss: 0.5192 - val_acc: 0.7464 - val_precision: 0.6950 - val_recall: 0.5593 - val_f1: 0.6074\n",
      "Training ended at 2018-05-08 01:59:53.477489\n",
      "Minutes elapsed: 0.959357\n",
      "Maximum accuracy at epoch 1 = 0.7464\n",
      "loss = 0.5190, accuracy = 0.7487,precision = 0.6988, recall = 0.5557,f1 = 0.6076\n"
     ]
    }
   ],
   "source": [
    "TEST_SPLIT = 0.2\n",
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]\n",
    "print(\"Q1_train\" , len(Q1_train))\n",
    "print(\"Q2_train\" , len(Q2_train))\n",
    "print(\"Q1_test\" , len(Q1_test))\n",
    "print(\"Q2_test\" , len(Q2_test))\n",
    "\n",
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question1)\n",
    "\n",
    "q1 = Dense(EMBEDDING_DIM,  activation='relu')(q1)\n",
    "q1 = Flatten()(q1)\n",
    "#q3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q3)\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question2)\n",
    "q2  = Dense(EMBEDDING_DIM,  activation='relu')(q2)\n",
    "q2 = Flatten()(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',precision,recall,f1])\n",
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))\n",
    "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc']))\n",
    "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))\n",
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "loss, accuracy,precision,recall, f1 = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
    "print('loss = {0:.4f}, accuracy = {1:.4f},precision = {2:.4f}, recall = {3:.4f},f1 = {4:.4f}'.format(loss, accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1_train 363861\n",
      "Q2_train 363861\n",
      "Q1_test 40429\n",
      "Q2_test 40429\n",
      "Starting training at 2018-05-08 02:12:01.258951\n",
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/1\n",
      " - 62s - loss: 0.5364 - acc: 0.7323 - precision: 0.6736 - recall: 0.5369 - f1: 0.5818 - val_loss: 0.5148 - val_acc: 0.7493 - val_precision: 0.6994 - val_recall: 0.5670 - val_f1: 0.6146\n",
      "Training ended at 2018-05-08 02:13:05.809903\n",
      "Minutes elapsed: 1.075837\n",
      "Maximum accuracy at epoch 1 = 0.7493\n",
      "loss = 0.5176, accuracy = 0.7506,precision = 0.7006, recall = 0.5612,f1 = 0.6116\n"
     ]
    }
   ],
   "source": [
    "TEST_SPLIT = 0.1\n",
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]\n",
    "print(\"Q1_train\" , len(Q1_train))\n",
    "print(\"Q2_train\" , len(Q2_train))\n",
    "print(\"Q1_test\" , len(Q1_test))\n",
    "print(\"Q2_test\" , len(Q2_test))\n",
    "\n",
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question1)\n",
    "\n",
    "q1 = Dense(EMBEDDING_DIM,  activation='relu')(q1)\n",
    "q1 = Flatten()(q1)\n",
    "#q3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q3)\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question2)\n",
    "q2  = Dense(EMBEDDING_DIM,  activation='relu')(q2)\n",
    "q2 = Flatten()(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',precision,recall,f1])\n",
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))\n",
    "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc']))\n",
    "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))\n",
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "loss, accuracy,precision,recall, f1 = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
    "print('loss = {0:.4f}, accuracy = {1:.4f},precision = {2:.4f}, recall = {3:.4f},f1 = {4:.4f}'.format(loss, accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
